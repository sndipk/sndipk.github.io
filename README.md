![Sandip Koli Portfolio](/assets/img/sandipcoverpic.png)
[Linkdin](https://linkedin.com/in/sndpk/)
[Github](https://github.com/sndipk)
# Education
## B.Tech | Computer Science | 2022 | 9.28 CGPA
## Diploma | Computer Engineering | 2019 | 67%

# Work Experience
## Data Analyst | TNT | JAN 2023 - OCT 2023

# Tableau Projects

## 1) Covid-19 

	•	Aim: Conducted comprehensive data analysis on COVID-19 data to identify trends, patterns, and insights that can inform public health policies and interventions.
	•	Problem: Analyzed infection rates, mortality rates, vaccination rates, and other key indicators to understand the impact of the COVID-19 pandemic on different regions and demographics.
	•	Activity: exploratory data analysis (EDA),  predictive modeling to forecast infection rates, hospitalization rates, and other relevant metrics based on historical data.
	•	Dataset: Utilized publicly available COVID-19 datasets from  World Health Organization (WHO)
	•	Included data on daily new cases, deaths, recoveries, testing rates, demographic information, and geographical location.
	•	Skills: data cleaning, preprocessing, and manipulation techniques, statistical analysis and data visualization tools, Python  relevant libraries like Pandas, NumPy, Matplotlib, and Seaborn for COVID-19 data analysis.

Description:
				The Tableau project on COVID-19 aimed to provide a comprehensive analysis of the pandemic's data to uncover trends and insights that could guide public health decisions. The primary goal was to analyze various metrics such as infection, mortality, and vaccination rates to gauge the pandemic's impact across different regions and populations.
The project involved extensive exploratory data analysis (EDA) and predictive modelling to estimate future trends in infection and hospitalization rates using historical data. The dataset comprised publicly available information from the World Health Organization (WHO), including daily updates on new cases, deaths, recoveries, testing rates, as well as demographic and geographical data.
Key skills employed in this project included data cleaning, preprocessing, manipulation, statistical analysis, and data visualization. Python and its associated libraries—Pandas, NumPy, Matplotlib, and Seaborn—were instrumental in the analysis of COVID-19 data, enabling the creation of insightful visualizations and models.
This project's findings were vital in understanding the dynamics of the pandemic and could significantly contribute to shaping effective health policies and interventions. By identifying patterns and predicting future occurrences, the project aimed to assist policymakers in making informed decisions to combat the ongoing health crisis.




## 2) IPL Data Analysis

        •	Aim: Conducted comprehensive data analysis on IPL (Indian Premier League) data to extract actionable insights and trends.
	•	Problem: Analyzed performance metrics, player statistics, team dynamics, and match outcomes to identify factors contributing to success in the IPL.
	•	Activity: Performed exploratory data analysis (EDA), sentiment analysis,predictive models 
	•	Dataset: Utilized publicly available IPL datasets from sources such as Kaggle, ESPN, or official IPL websites which Included data on match results, player statistics, team standings, venue details, and other relevant attributes.
	•	Skills:data cleaning, preprocessing, and manipulation techniques,statistical analysis and data visualization tools, machine learning algorithms for predictive modeling, Python programming and relevant libraries such as Pandas, NumPy, Matplotlib, and Seaborn, communication skills.

Description:
				The project titled "IPL Data Analysis" was an extensive endeavor aimed at delving into the vast data generated by the Indian Premier League (IPL) to uncover valuable insights and patterns that could influence strategic decisions in the sport. The primary objective was to conduct a thorough analysis of IPL data, which included a variety of performance metrics, player statistics, team dynamics, and match outcomes. This analysis was pivotal in pinpointing the elements that contribute to a team's success within the league.
The problem tackled involved dissecting and interpreting complex datasets to understand the underlying factors that lead to victories or defeats in the IPL. By scrutinizing performance indicators and player statistics, the project sought to reveal the intricate interplay between individual prowess and team synergy. Additionally, the outcomes of matches were analyzed to discern any recurring themes or strategies that consistently yielded positive results.
A multi-faceted approach was adopted for the activity phase of the project. This included:
	•	Exploratory Data Analysis (EDA): A foundational step that involved sifting through the data to identify patterns, anomalies, correlations, and trends.
	•	Sentiment Analysis: An innovative angle that examined the emotions and opinions expressed by fans and commentators, providing a unique perspective on the game's impact.
	•	Predictive Models: The creation of sophisticated models that could forecast future performances and outcomes based on historical data.
The dataset was a compilation of publicly available IPL data sourced from reputable platforms like Kaggle, ESPN, and official IPL websites. It encompassed a comprehensive range of information, including match results, individual player statistics, team standings, venue specifics, and other pertinent details that enriched the analysis.
The skills employed in this project were diverse and indicative of a high level of expertise in data science. They included:
	•	Data Cleaning, Preprocessing, and Manipulation: Techniques that ensured the data was accurate, consistent, and ready for analysis.
	•	Statistical Analysis and Data Visualization: The use of tools to interpret the data and present it in a visually compelling manner.
	•	Machine Learning Algorithms: The application of advanced algorithms for predictive modeling, which could simulate potential future scenarios.
	•	Programming Proficiency: Demonstrated through the use of Python and its associated libraries such as Pandas, NumPy, Matplotlib, and Seaborn, which are essential for handling, analyzing, and visualizing data.
	•	Communication Skills: The ability to articulate findings and insights clearly and persuasively to stakeholders.
In conclusion, the "IPL Data Analysis" project was a testament to the power of data in transforming sports analytics. By leveraging cutting-edge techniques and tools, the project provided a deeper understanding of the IPL, paving the way for data-driven strategies that could potentially revolutionize the game.


## 3) Agriculture product prediction

	•       Aim: To develop a predictive model for agriculture product production that accurately forecasts rice, wheat, or vegetable yields in various countries over the next few years.
	•	Problem: create a model to predict agriculture production in countries in the next few years for rice, wheat or vegetables
	•	Activity: Perform regression analysis(linear regression, logistic regression) to predict trends
	•	DataSet: Kaggle;s world_foods Dataset
	•	Skills: Data Cleaning, Python, Data/Statistical analysis, Visulisatizationn, Trends, Modeling, Communication

Description: 
		The project Agriculture Product Prediction is a comprehensive Tableau venture aimed at developing a predictive model to forecast agricultural product yields, specifically rice, wheat, and vegetables, across various countries in the upcoming years. The model's objective is to provide accurate predictions that can assist policymakers, farmers, and stakeholders in making informed decisions to optimize production and ensure food security.
Project Description:
Aim: The primary goal is to construct a predictive model that leverages historical data to forecast future yields of key agricultural products. By predicting the production of rice, wheat, and vegetables, the model will serve as a crucial tool for planning and resource allocation in the agricultural sector.
Problem Statement: The challenge lies in creating a robust model capable of predicting agricultural production with high accuracy. This model must account for the complex interplay of factors that influence crop yields, such as climate variability, soil quality, farming practices, and socio-economic conditions.
Activity: To address this challenge, the project will employ regression analysis, including linear and logistic regression techniques, to identify trends and patterns within the data. This statistical approach will help in understanding the relationships between various predictive factors and crop yields.
Data Set: The foundation of this predictive model is the "world_foods" dataset from Kaggle, which contains extensive data on global food production. This dataset will be meticulously cleaned and preprocessed to ensure the quality and reliability of the predictions.
Skills Required: The project demands a diverse set of skills, including:
	•	Data Cleaning: Ensuring the dataset is free of inconsistencies and errors to maintain the integrity of the analysis.
	•	Python Programming: Utilizing Python for data manipulation, analysis, and model development.
	•	Data/Statistical Analysis: Applying statistical methods to understand data characteristics and extract meaningful insights.
	•	Visualization: Creating intuitive and informative visualizations in Tableau to communicate findings and trends effectively.
	•	Trend Analysis: Identifying and interpreting trends to predict future outcomes.
	•	Modeling: Building and refining predictive models to achieve high accuracy.
	•	Communication: Clearly conveying complex information and results to a non-technical audience.
The project's success hinges on the seamless integration of these skills to create a model that not only predicts agricultural production but also provides actionable insights for improving food security and sustainability. The predictive model developed through this project has the potential to revolutionize agricultural planning and management, leading to more efficient and sustainable practices worldwide.


## 4) Loan Approval Prediction -

	Aim: To accurately predict whether a loan request will be approved or denied, based on the details provided by the applicant

	•	 gather loan application data,
	•	 explore and analyse the data
	•	preprocess and transform the data
	•	select relevant feature for classification and train the model 
	•	use the trained model to predict loan approval
	
	Description: 
				The project Loan Approval Prediction is a sophisticated Tableau dashboard designed to streamline the decision-making process in the banking sector. The aim is to leverage predictive analytics to determine the likelihood of loan approval for applicants. This is achieved through a series of methodical steps, each contributing to the model's accuracy and reliability.
Data Gathering: The first phase involves collecting comprehensive loan application data from potential borrowers. This dataset includes personal, financial, and employment information, which forms the backbone of the predictive analysis.
• Data Exploration and Analysis: With the data in hand, the next step is to conduct an exploratory analysis. This involves scrutinizing the data for patterns, correlations, and trends that could influence loan approval decisions. Tableau's interactive visualizations aid in identifying key factors that are indicative of successful loan repayments.
• Data Preprocessing and Transformation: Raw data often contains inconsistencies, missing values, and outliers that can skew predictions. Therefore, preprocessing is crucial. This stage cleanses the data, handles missing values, encodes categorical variables, and normalizes numerical values to prepare a robust dataset for modeling.
• Feature Selection and Model Training: Not all data points are equally informative. Selecting the most relevant features is vital for an effective model. Using Tableau, analysts can visualize the impact of various features on loan approval outcomes. Once the features are selected, a classification model is trained to discern patterns that lead to loan approvals or denials.
• Prediction: The trained model is then employed to predict loan approvals. Applicants' details are fed into the model, which then calculates the probability of approval. The Tableau dashboard presents these predictions in an accessible format, allowing loan officers to make informed decisions quickly.
• Outcome: The culmination of this project is a Tableau dashboard that not only predicts loan approvals with high accuracy but also provides insights into the factors influencing these decisions. It serves as a decision support tool that enhances the efficiency and objectivity of the loan approval process.
This project exemplifies the power of data analytics in financial decision-making, offering a transparent, data-driven approach to loan approvals that benefits both lenders and borrowers alike.


## 5) Energy Consumption analysis

	•	Aim: Analyzed total energy consumption and costs, identifying top providers and peak usage patterns.
	•	Problem: Addressed the challenge of optimizing energy usage and reducing expenses in urban settings.
	•	Activity: Conducted a comprehensive study of energy data, spanning multiple years and cities.
	•	Dataset: Utilized records detailing monthly energy use, costs, provider rankings, and consumption categories.
	•	Skills: Applied statistical analysis and data visualization to interpret complex energy consumption patterns.


	Description:
		     This Tableau project, titled Energy Consumption Analysis, embarks on a critical mission to dissect and understand the intricate web of energy consumption and its associated costs. The aim is to shed light on the dominant energy providers and unravel the patterns of peak energy usage.
Aim: The project's primary objective is to meticulously analyze the total energy consumption and the financial burden it imposes. It seeks to pinpoint the leading energy providers and decode the trends of peak usage times and seasons.
Problem: Urban landscapes face the pressing issue of optimizing energy utilization while curtailing the soaring expenses that accompany it. This project tackles this challenge head-on, proposing data-driven solutions to enhance energy efficiency in metropolitan environments.
Activity: A thorough investigation was undertaken, encompassing a vast array of energy data collected over several years and across a multitude of cities. This extensive study aimed to capture a holistic view of the energy landscape.
Dataset: The backbone of this analysis is a robust dataset that chronicles monthly energy usage, the costs incurred, the hierarchy of energy providers, and the various categories of consumption. This dataset serves as a crucial resource for uncovering patterns and insights.
Skills: The project harnesses the power of statistical analysis and the art of data visualization to make sense of complex energy consumption patterns. These skills are pivotal in transforming raw data into actionable intelligence.
Through this project, stakeholders can gain a comprehensive understanding of energy dynamics, which is instrumental in making informed decisions about energy management and sustainability strategies. The insights gleaned from this analysis can lead to significant cost savings and contribute to the broader goal of energy conservation. By identifying inefficiencies and potential areas for improvement, this project paves the way for a more energy-conscious future.

				

## 6) SampleSuperstore

	•	Aim: Analyzed the Sample Superstore dataset to identify sales trends and improve business strategies.
	•	Problem: Addressed the challenge of declining sales in certain categories and regions.
	•	Activity: Conducted a thorough data exploration and visualization exercise to uncover insights.
	•	Dataset: Utilized the Sample Superstore dataset, which includes fields like category, subcategory, sales, region, and cost.
	•	Skills: Applied data analysis and visualization skills using tools like Excel and Tableau to interpret the dataset.

Description:
			The SampleSuperstore Tableau project was a comprehensive analytical endeavor aimed at enhancing business strategies through the identification of sales trends. The project's primary objective was to scrutinize the Sample Superstore dataset, a rich repository of sales data, to discern patterns and insights that could inform better decision-making processes.
Project Overview:
	•	Aim: The project was designed to analyze the Sample Superstore dataset meticulously to detect sales trends that could potentially revamp existing business strategies.
	•	Problem: A notable decline in sales across specific categories and regions was observed, posing a significant challenge that the project sought to address.
	•	Activity: An extensive data exploration and visualization exercise was undertaken. This involved delving into the dataset to extract meaningful insights that could explain the sales downtrend.
	•	Dataset: The Sample Superstore dataset was at the heart of this analysis. It encompassed a variety of fields such as category, subcategory, sales, region, and cost, providing a holistic view of the company's operations.
	•	Skills: The project harnessed data analysis and visualization skills, employing tools like Excel and Tableau. These tools were instrumental in interpreting the dataset and transforming raw data into actionable intelligence.
Detailed Description: The project began with the ingestion of the Sample Superstore dataset into Tableau, where initial data cleaning and preprocessing were performed. This step was crucial to ensure the accuracy of the subsequent analysis. Once the data was prepared, a series of exploratory data analyses were conducted. This included examining sales performance across different categories and subcategories, assessing the profitability of various regions, and identifying cost patterns that could influence sales outcomes.
Visualizations played a pivotal role in the project, with Tableau being used to create a range of charts and graphs that illustrated the data in an accessible manner. From heat maps highlighting regional sales hotspots to line charts depicting sales trends over time, each visualization offered a unique perspective on the data.
One of the key findings from the analysis was the identification of underperforming categories and regions. This insight enabled the formulation of targeted strategies aimed at revitalizing these areas. For instance, marketing efforts could be intensified in regions with lower sales, and product offerings could be optimized in categories that were not performing well.
Moreover, the project also shed light on the correlation between cost and sales. By analyzing the cost structure and its impact on sales, the project provided recommendations on pricing strategies that could enhance profitability without compromising on sales volume.
In conclusion, the SampleSuperstore Tableau project was a testament to the power of data analysis and visualization in uncovering insights that drive business growth. Through a methodical approach that combined technical skills with business acumen, the project delivered a set of actionable recommendations that could lead to improved sales performance and strategic business development.



## 7) Car details 

	•	Aim: Aimed to analyze the car market trends and predict car prices.
	•	Problem: Addressed the challenge of accurately predicting used car prices based on various features.
	•	Activity: Collected and processed data on used cars, including details like fuel type, name, owner history, seller type, transmission type, year of manufacture, kilometers driven, and selling price.
	•	Dataset: Utilized the CarDekho dataset, which comprised information such as fuel type, car name, owner type, seller type, transmission, year of manufacture, distance driven, and selling price.
	•	Skills: Applied data cleaning, preprocessing, exploratory data analysis, and machine learning techniques to derive insights and build predictive models.

Description: 
			The Tableau project on car details is a comprehensive analysis aimed at understanding the dynamics of the car market and predicting car prices with a high degree of accuracy. The project's primary goal is to provide valuable insights that can assist potential buyers, sellers, and manufacturers in making informed decisions.
Project Overview:
	•	Aim: The project is designed to dissect car market trends methodically and forecast car prices. By doing so, it seeks to offer a granular view of the factors influencing car valuations.
	•	Problem Statement: The core challenge tackled by this project is the precise prediction of used car prices. This involves considering a multitude of variables that can affect a car's market value.
	•	Activities Undertaken:
	•	Data Collection: A robust dataset was curated, encompassing a wide array of used car details. This dataset serves as the foundation for all subsequent analysis.
	•	Data Processing: The collected data underwent meticulous cleaning and preprocessing to ensure accuracy and reliability for the analysis phase.
	•	Exploratory Data Analysis (EDA): Through EDA, the project identified patterns, anomalies, correlations, and trends within the data, providing a deeper understanding of the used car market.
	•	Predictive Modeling: Leveraging machine learning algorithms, the project developed models capable of predicting used car prices based on the insights derived from the EDA.
Dataset Details: The CarDekho dataset was instrumental in this project. It is a rich repository of information, including:
	•	Fuel Type: Categorizing cars based on their fuel consumption (e.g., petrol, diesel, CNG).
	•	Car Name: The make and model of the car.
	•	Owner Type: The ownership history (e.g., first-owner, second-owner).
	•	Seller Type: The type of seller (e.g., individual, dealer).
	•	Transmission Type: The gearbox configuration (e.g., manual, automatic).
	•	Year of Manufacture: The production year of the car.
	•	Kilometers Driven: The total distance covered by the car.
	•	Selling Price: The price at which the car is being sold.
Skills and Techniques: The project team applied a range of skills and techniques to achieve its objectives:
	•	Data Cleaning: Ensuring the dataset is free from inaccuracies and inconsistencies.
	•	Data Preprocessing: Transforming raw data into a format suitable for analysis.
	•	Exploratory Data Analysis: Utilizing statistical graphics, plots, and information graphics to explore the data.
	•	Machine Learning: Implementing algorithms to create predictive models based on the dataset.
Conclusion: This Tableau project stands as a testament to the power of data analysis and machine learning in the automotive industry. It not only addresses the complexities of car price prediction but also sets a benchmark for similar studies in the future. The insights and models derived from this project could be pivotal for stakeholders in the used car market, enabling them to make more strategic decisions backed by data-driven evidence.




